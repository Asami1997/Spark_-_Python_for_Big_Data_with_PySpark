{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('streaming').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('paysim.csv',inferSchema = True,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for them now\n",
    "df = df.drop('isFraud','isFlaggedFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step maps a unit of time\n",
    "df.groupby('step').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a step represnets a unit of time\n",
    "steps = df.select('step').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in steps :\n",
    "     print(step[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we just want to simulate as if i am getting a new file for each step combines so we can later ingest it\n",
    "\n",
    "here i am combing all the data related to each distinct step into one file so we can ingest it later\n",
    "\"\"\"\n",
    "for step in steps :\n",
    "    df_current = df.filter(df.step == step[0])\n",
    "    #save it in 1 file\n",
    "    df_current.coalesce(1).write.mode('append').option(\"header\" , \"true\").csv('data/paysim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to get the schema\n",
    "part = spark.read.csv('/paysim/part-00000-8ecc5d10-9134-4a1c-93e2-b761f3a997f7-c000.csv',\n",
    "                         inferSchema = True,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = part.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streaming dataframe\n",
    "#limit it to one file per trigger\n",
    "#lets read each file one by one as if it was a stream\n",
    "streaming_df = (spark.readStream.schema(data_schema)\n",
    "               .option('maxFilePerTrigger',1)\n",
    "               .csv('/data/paysim/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple transformation\n",
    "dest_count = streaming_df.groupBy('nameDest').count().orderBy('count',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now since we have the streaming dataframe and the transformation defined\n",
    "# create the destination sink , for now ill set it to memory\n",
    "\n",
    "active_query = (dest_count.writeStream.queryName('dest_count')\n",
    "                .format('memory')\n",
    "                .outputMode('complete')\n",
    "                .start())\n",
    "\n",
    "# this is just for us keep looping untill these is new data\n",
    "import time\n",
    "\n",
    "for x in range(50):\n",
    "    _df = spark.sql(\"select * from dest_count\")\n",
    "    \n",
    "    if _df.count() > 0:\n",
    "        _df.show(10)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the active streams \n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the stream\n",
    "active_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
